# COMS 6998 Mid-term Seminar Blog
# Deep Learning in Option Pricing and Optimal Stopping Problem

This blog shows what I've learnt from the two papers about deep learning in option pricing.

# Motivation
Option is an important financial derivatives which

High-dimensional optimal stopping problems are notoriously difficult to solve due to the well-known curse of dimensionality. The input data is often high dimension since there might be multiple underlying assets in a hedging portfolio so that almost all of them do not have calculation methods to gain a explicit answer. 
Lots of research try to use different input design and analyse approximation methods to solve it while the deep learning offers not only an approximation of an optimal strategy but also the optimal expected payoff associated to the considered optimal stopping problem. 

# Background
**1. For optimal stopping problem:** 
- The previous work is trying to use delicate input to better calculate the optimal stopping while the deep learning algorithm is insensitive to the input. The uniform sampling is more preferable.
- Avoid the problem of unaffordable computing time for high-dimensional problem
- Detection of wrong inputs with noise

**2. For pricing/calibration of options:**
- Besides geometric Brownian motion(GBM), variance gamma(VG) model is also commonly use in option pricing so the author want to explore the potential of combine those two.
- Other than the simple European options and American options, Barrier options are more complicated with several more condition to execute and the author also consider about pricing those type of option.

# Paper 1: Deep Learning in Optimal Stopping
## 1.1 Introduction
The paper is mainly about using deep learning algorithms for ranking response surfaces with applications to optimal stopping problems. The surface ranking problem consists in assigning the index of the minimal surface to every input x in the entire (usually continuous) space X, i.e. finding the classifer

![image](https://user-images.githubusercontent.com/55861244/100691900-b06c7000-3357-11eb-9f54-e32590400a7a.png)

The function we want to minimize is a priori unknown but can be noisily sampled, i.e. for any x, one can access the function through its stochastic sampler Y:

![image](https://user-images.githubusercontent.com/55861244/100692175-430d0f00-3358-11eb-9cc3-b9a553a48147.png)

where the last part epsilons are independen random variables with mean zero and variance ![image](https://user-images.githubusercontent.com/55861244/100694501-3b9c3480-335d-11eb-82a2-0f27201a3315.png).
Since we want to estimate C(x) using eep learning algorithms we evaluate the differences using the following loss metric:

![image](https://user-images.githubusercontent.com/55861244/100692361-aac35a00-3358-11eb-8376-50d92de2e635.png)

where the lambda(dx) is a probability measure on X specifying the relative importance of ranking different regions. Here when we evaluate the training accuracy and the generalization accuracy, the loss metric is using the uniform measure ![image](https://user-images.githubusercontent.com/55861244/100694560-55d61280-335d-11eb-9496-566bab3ef05e.png).

## 1.2 Neural Networks and Deep Learning Algorithms
The input for the the neural networks is 
![image](https://user-images.githubusercontent.com/55861244/100694596-671f1f00-335d-11eb-8292-b4ae6391b391.png)
where J corresponds to the number of points and d is the dimensionality of the problem. And the output should take the form:

![image](https://user-images.githubusercontent.com/55861244/100692685-579dd700-3359-11eb-98ad-4cfb4c4c0066.png)

where L is the number of response surfaces, and pjl is the probability that the l th surface is the minimal at the j th point.
The author uses two types of neural networks which is feed-forward NNs and UNet:

![image](https://user-images.githubusercontent.com/55861244/100692967-edd1fd00-3359-11eb-8f5f-c35d5d5a8d05.png)

For the optimizer, the author uses the commonly used stochastic gradient descent method. For activation function, the author used ReLu, sigmoid and softmax function.

## 1.3 Numerical Experiments
### 1.3.1 One-dimensional example
The main purpose of this example is to show that input data generated by uniform grids UNIF(or UNIF+NL) gain higher generalization accuracy than sequential design SD(or SD+NL).
Author did this example is trying to see the deep learning performance with training data generated on uniform grids or the points produced by sequential design.
The author set L=2, X=\[0,1\] and define the noisy responses Y1(x) and Y2(x) as 

![image](https://user-images.githubusercontent.com/55861244/100693431-e19a6f80-335a-11eb-889b-676c4cfb0f0f.png)

Then the true ranking classifier C(x) is comuted as 

![image](https://user-images.githubusercontent.com/55861244/100693636-4e156e80-335b-11eb-9591-7c4f78dcf4dc.png)

Let M be the size of traning data, they conduct their experiments under M=128,256,512 and number of neurons in each hidden layer is set at M/8. For this binary classification problem, they use signoid activation function in last layer and trained the network for 1500 epochs with updating each gradient using M/2 data. They finally get the below result:

![image](https://user-images.githubusercontent.com/55861244/100693951-f88d9180-335b-11eb-8609-9d352cdfddda.png)

The above results show that:
1. for NNs trained by clean data, the training accuracy is higher than the generalization accuracy while for NNs trained by noisy data, the generalization accuracy is higher. This shows that although the training data with noicy labels do not contain 100% accurate information, networks still try to eliminate their effect and adjust them by not training them correctly.
2. The points generating by sequential design contain more errors since they are more concentrated around the true boundaries and fake boundary. This leads to a large number of mis-labeled points and brings lower signal-to-noise ratio.
3. There exists a threshold on the proportion of error in training data so that they can be auto-detect and will not influence the network's predicting accuracy.
After this example, they believes that it is better to use training data generating by uniform grids and there is no need to do the sequential design for deep learning input.
### 1.3.2 Two-dimensional example
The two-dimensional example is aim to study the sensitivity of deep learning algorithms to noisy labels, sampling locations and budget. So the author train the network for a more complex setting with L=5 surfaces and a 2D input space X=\[-2,2\]^2.
The surface, response and training accuracy is showed below:

![image](https://user-images.githubusercontent.com/55861244/100695164-a732d180-335e-11eb-926b-3b8485b4e2ea.png)

The author believes that considering ranking response surfaces as image segmentation allows one to use a broad class of deep neural networks. Since there are no theoretical results in literature indicating which type of architecture works best for specific applications, they tried feed-forward NNs and UNet. 
In this example, they implement the UNet architecture using M=576 uniform grid points with noisy labels, and gain a generalization accuracy of 96.44%, presenting a better performance than feed-forward NNs (corresponding to the 95.1% in table 3), with comparable training time.


### 1.3.3 Ten-dimensional example
The main purpose of this example is to show the robustness of deep learning algorithms. Non-learning algorithms have difficulty in computational time for high dimension. For those example, when NN is trained by clean data without noise, the train accuracy is better while if it is trained by noisy data, the generalization accuracy is better.
In this example 

## 1.4 Bermudan Option Pricing

## 1.5 Observations and Insights
- Deep learning method perform quite well with uniform inputs which seems no need to design a special method to generate input and it can also auto-detect the noisy part although it still perform better for the clean data.
- It also provides a solution for high-dimensional problem within affordable computing time.

# Paper 2: Deep Learning in Pricing/Calibration of Vanilla/Exotic Options
## 2.1 Introduction

## 2.2 Neural Networks and Deep Learning Algorithms

## 2.3 Numerical Experiments

## 2.4 Observations and Insights
- Find out that RNNs perform pretty well and also provide a cheap and faster method than FFNNs to price American option. The model can be used for derivative pricing for market parameters both within and reasonably outside the initial range of the training set with very low error.
- Provide a way to not only price the European options and American options, but also the more complicate Barrier options. The model perform well under in-sample test data as well as longer maturity extrapolation data.


```markdown

**Bold** and _Italic_ and `Code` text
```


# Conclusion


# Reference:
1.[Deep learning for ranking response surfaces with applications to optimal stopping problems](https://www-tandfonline-com.ezproxy.cul.columbia.edu/doi/full/10.1080/14697688.2020.1741669)
2.[Supervised Deep Neural Networks (DNNs) for Pricing/Calibration of Vanilla/Exotic Options Under Various Different Processes](https://arxiv.org/abs/1902.05810)

## Contact Me
If you have any questions on this page, please contact me via rn2498@columbia.edu.

This the blog for midterm seminar. My teammate is Yuting Liu.
