## COMS 6998 Mid-term Seminar Blog
# Deep Learning in Option Pricing and Optimal Stopping Problem

This blog shows what I've learnt from the two papers about deep learning in option pricing.

# Motivation
High-dimensional optimal stopping problems are notoriously difficult to solve due to the well-known curse of dimensionality. The input data is often high dimensional since there might be multiple underlying assets in a hedging portfolio so that almost all of them do not have calculation methods to gain an explicit answer. Lots of research tries to use different input design and analyse approximation methods to solve it while deep learning offers not only an approximation of an optimal strategy but also the optimal expected payoff associated with the considered optimal stopping problem.

Also, pricing and calibration of different types of options can be expensive and time demanding while every second means money in the market. Therefore, using deep learning algorithms to provide an inexpensive computationally way to training and predicting options prices is a natural idea. The neural networks architecture results in speed-ups of many orders of magnitude, making it practical and easy to implement and use.

# Background
**1. For optimal stopping problem:** 
- The previous work is trying to use delicate input to better calculate the optimal stopping while the deep learning algorithm is insensitive to the input. The uniform sampling is more preferable.
- The researcher wants to avoid the problem of unaffordable computing time for high-dimensional problems.
- The researcher wants methods that can auto-detect wrong inputs with noise.

**2. For pricing/calibration of options:**
- Besides geometric Brownian motion(GBM), variance gamma(VG) model is also commonly used in option pricing so the author wants to explore the potential of combining those two.
- Other than the simple European options and American options, Barrier options are more complicated with several more conditions to execute and the author also considers pricing those types of options.

# Paper 1: Deep Learning in Optimal Stopping
## 1.1 Introduction
The paper is mainly about using deep learning algorithms for ranking response surfaces with applications to optimal stopping problems. The surface ranking problem consists in assigning the index of the minimal surface to every input x in the entire (usually continuous) space X, i.e. finding the classifer

![image](https://user-images.githubusercontent.com/55861244/100691900-b06c7000-3357-11eb-9f54-e32590400a7a.png)

The function we want to minimize is a priori unknown but can be noisily sampled, i.e. for any x, one can access the function through its stochastic sampler Y:

![image](https://user-images.githubusercontent.com/55861244/100692175-430d0f00-3358-11eb-9cc3-b9a553a48147.png)

where the last part epsilons are independen random variables with mean zero and variance ![image](https://user-images.githubusercontent.com/55861244/100694501-3b9c3480-335d-11eb-82a2-0f27201a3315.png).
Since we want to estimate C(x) using eep learning algorithms we evaluate the differences using the following loss metric:

![image](https://user-images.githubusercontent.com/55861244/100692361-aac35a00-3358-11eb-8376-50d92de2e635.png)

where the lambda(dx) is a probability measure on X specifying the relative importance of ranking different regions. Here when we evaluate the training accuracy and the generalization accuracy, the loss metric is using the uniform measure ![image](https://user-images.githubusercontent.com/55861244/100694560-55d61280-335d-11eb-9496-566bab3ef05e.png).

## 1.2 Neural Networks and Deep Learning Algorithms
### 1.2.1 Input and Output
The input for the the neural networks is 
![image](https://user-images.githubusercontent.com/55861244/100694596-671f1f00-335d-11eb-8292-b4ae6391b391.png)
where J corresponds to the number of points and d is the dimensionality of the problem. And the output should take the form:

![image](https://user-images.githubusercontent.com/55861244/100692685-579dd700-3359-11eb-98ad-4cfb4c4c0066.png)

where L is the number of response surfaces, and pjl is the probability that the l th surface is the minimal at the j th point.
### 1.2.2 Architecture
The author uses two types of neural networks which is feed-forward NNs and UNet:

![image](https://user-images.githubusercontent.com/55861244/100692967-edd1fd00-3359-11eb-8f5f-c35d5d5a8d05.png)

### 1.2.3 Optimizer and Activation Function
For the optimizer, the author uses the commonly used stochastic gradient descent method. For activation function, the author used ReLu, sigmoid and softmax function.

## 1.3 Numerical Experiments
### 1.3.1 One-dimensional example
The main purpose of this example is to show that input data generated by uniform grids UNIF(or UNIF+NL) gain higher generalization accuracy than sequential design SD(or SD+NL).

This example is trying to show the deep learning performance with training data generated on uniform grids or the points produced by sequential design.

The author set L=2, X=\[0,1\] and define the noisy responses Y1(x) and Y2(x) as 

![image](https://user-images.githubusercontent.com/55861244/100693431-e19a6f80-335a-11eb-889b-676c4cfb0f0f.png)

Then the true ranking classifier C(x) is comuted as 

![image](https://user-images.githubusercontent.com/55861244/100693636-4e156e80-335b-11eb-9591-7c4f78dcf4dc.png)

Let M be the size of training data, they conduct their experiments under M=128,256,512 and number of neurons in each hidden layer is set at M/8. For this binary classification problem, they used sigmoid activation function in the last layer and trained the network for 1500 epochs with updating each gradient using M/2 data. 

They finally get the below result:

![image](https://user-images.githubusercontent.com/55861244/100693951-f88d9180-335b-11eb-8609-9d352cdfddda.png)

The above results show that:
1. For NNs trained by clean data, the training accuracy is higher than the generalization accuracy while for NNs trained by noisy data, the generalization accuracy is higher. This shows that although the training data with noisy labels do not contain 100% accurate information, networks still try to eliminate their effect and adjust them by not training them correctly.
2. The points generated by sequential design contain more errors since they are more concentrated around the true boundaries and fake boundary 0. This leads to a large number of mis-labeled points and brings lower signal-to-noise ratio.
3. There exists a threshold on the proportion of error in training data so that they can be auto-detect and will not influence the network's predicting accuracy.After this example, they believe that it is better to use training data generated by uniform grids and there is no need to do the sequential design for deep learning input.

### 1.3.2 Two-dimensional example
The two-dimensional example aims to study the sensitivity of deep learning algorithms to noisy labels, sampling locations and budget. So the author trains the network for a more complex setting with L=5 surfaces and a 2D input space X=\[-2,2\]^2.

The surface, response and training accuracy is showed below:

![image](https://user-images.githubusercontent.com/55861244/100695164-a732d180-335e-11eb-926b-3b8485b4e2ea.png)

The author believes that considering ranking response surfaces as image segmentation allows one to use a broad class of deep neural networks. Since there are no theoretical results in literature indicating which type of architecture works best for specific applications, they tried feed-forward NNs and UNet. 

In this example, they implement the UNet architecture using M=576 uniform grid points with noisy labels, and gain a generalization accuracy of 96.44%, presenting a better performance than feed-forward NNs (corresponding to the 95.1% in table 3), with comparable training time.


### 1.3.3 Ten-dimensional example
The main purpose of this example is to show the robustness of deep learning algorithms. Non-learning algorithms have difficulty in computational time for high dimensions. For example, when NN is trained by clean data without noise, the train accuracy is better while if it is trained by noisy data, the generalization accuracy is better.

In this example, the author uses L=3 surfaces and X=[-1,1]^d with d=10. The surfaces they use include embedded Harmann 6-D function, rescaled Styblinski-Tang function, and rescaled Trid function as described in below table:

![image](https://user-images.githubusercontent.com/55861244/100699253-70fa4f80-3368-11eb-9e25-5e2239cd1fbe.png)

and gain the below training accuracy:

![image](https://user-images.githubusercontent.com/55861244/100699268-7f486b80-3368-11eb-8b95-38ffa3fc780d.png)

## 1.4 Bermudan Option Pricing
The author then use the below algorithm to pricing Bermudan Option:

![image](https://user-images.githubusercontent.com/55861244/100699708-b3705c00-3369-11eb-8bd9-1ede2b884f12.png)

They use the below parameters from the previous research as below:

![image](https://user-images.githubusercontent.com/55861244/100699855-0fd37b80-336a-11eb-95a0-2f166abc66b3.png)

And find out that not only the algorithms provide an result for high-dimensional problem within an affordable time, but also once it is done, if the new dataset is not drastically different form the original dataset (which is very likely), the pre-trained network will already learn some features that are relevant to the current classification problem and only fine-tuning is needed.

![image](https://user-images.githubusercontent.com/55861244/100700058-8e301d80-336a-11eb-95e2-e1c1a3c46c01.png)

## 1.5 Observations and Insights
- Deep learning methods perform quite well with uniform inputs which seems no need to design a special method to generate input and it can also auto-detect the noisy part although it still performs better for the clean data.
- It also provides a solution for high-dimensional problems within affordable computing time.

# Paper 2: Deep Learning in Pricing/Calibration of Vanilla/Exotic Options
## 2.1 Introduction
The authors apply supervised deep neural networks (DNNs) for pricing and calibration of both vanilla and exotic options under both diffusion and pure jump processes with and without stochastic volatility. 
The author consider four different process:
1. Geometric Brownian motion (GBM) 
2. GBM + stochastic arrival (GBMSA) a.k.a Heston stochastic volatility model
3. Variance gamma (VG) model
4. VG + stochastic arrival （VGSA)


## 2.2 Neural Networks and Deep Learning Algorithms
### 2.2.1 Input and Output
They used sample sizes of 300,000 points for training and 60,000 points for validation.

For the input of the network, they use the parameter matrix:

![image](https://user-images.githubusercontent.com/55861244/100696806-66d55280-3362-11eb-8388-0aab654bbb96.png)

where all the parameters has the range according to the reality:
- Product and market parameters:

![image](https://user-images.githubusercontent.com/55861244/100697283-98025280-3363-11eb-9073-57927e66a4b6.png)

- Parameters for different model:

![image](https://user-images.githubusercontent.com/55861244/100696856-85d3e480-3362-11eb-8076-66623bce6c26.png)

- **For European Options**, the output y=EC/K, and EC is gained using Black-Merton-Scholes formula for GBM process, while EC is obtained by using the Fast Fourier Transform(FFT) algorithm for corresponding input parameter matrices X for VG, GBMSA, VGSA models.
- **For Barrier Options**, since we have put-call parity and relationships between barrier options, the author only considers the Up-and-Out Put Options (UOP). The output is y=UOP/K. UOP is obtained by using the closed form solution for GBM while it is obtained by using Monte Carlo procedure for their corresponding input parameter matrices X for other models.
- **For American Options**, y=AJZ/K where AJZ is obtained by using the Ju-Zhong Approximation.

### 2.2.2 Architecture
They used feed-forward Neural Networks and Convolutional Neural Networks.

After experiments, they choose 120 neurons per layer and 4 layers, which provides the lowest RMSE in their experiments.

### 2.2.3 Optimizer and Activation Function
For the optimizer, they first tried SGD but suffered from the slow training time and after trying RMSprop and Adam, they finally chose Adam as the optimizer. In their Neural Networks, they use MSE as the loss function. After their numerical experiments, they found that the activation of 4 layers with first 3 leaky and last one elu gained the best performance.

## 2.3 Numerical Experiments
### 2.3.1 Validation
In order to validate the model, they check trained model for three cases:
1. **Interpolation(IS):** In this case, they just validate it for interpolated points on our testing set.
2. **Deep-out-of-the-money(DOM):** In this case, they keep everything else fixed, and test how the trained models behave when S0/K is between 0.6 and 0.8.
3. **Longer maturity(LM):**  In this case, they keep everything else fixed, and test how the trained models behave when the maturity is between 3 years and 5 years.

And obtained the below results:
- **For European Options:**

![image](https://user-images.githubusercontent.com/55861244/100698007-6d18fe00-3365-11eb-80c5-33c71667c5f5.png)

- **For Barrier Options:**

![image](https://user-images.githubusercontent.com/55861244/100698082-a05b8d00-3365-11eb-922a-0c00e9736560.png)

- **For American Options:**

![image](https://user-images.githubusercontent.com/55861244/100698692-144a6500-3367-11eb-8f1e-f8afbd748bd8.png)

They observed that with an increase in depth in layers and neurons per layer the accuracy of the predictions increases, but eventually diminishes. Once the training is done, the model can be used for derivative pricing for market parameters both within and reasonably outside the initial range of the training set with very low error.

### 2.3.2 FFNNs VS RNNs
- Although feedforward neural networks(FFNN) perform well but it is quite expensive to generate labels for American options. Knowing American options are strongly path-dependent options, we might benefit from recurrent neural networks(RNN) in pricing them. 
- According to the numerical results, the MSE value of FFNN model is slightly smaller than that of the RNN model. 
- On the other hand, there is a big gap between training times of the two models. In our experiment training FFNNs took 50% more time than RNNs. 

## 2.4 Observations and Insights
- Find out that RNNs perform pretty well and also provide a cheaper and faster method than FFNNs to price American options. The model can be used for derivative pricing for market parameters both within and reasonably outside the initial range of the training set with very low error.
- Provide a way to not only price the European options and American options, but also the more complicated Barrier options. The model performs well under in-sample test data as well as longer maturity extrapolation data.


# Conclusion
- **For the paper 1**, we learn that deep learning algorithms can be used for ranking response surfaces problems and finding optimal stopping problems, especially for high-dimensional problems. In this application, deep learning algorithms are insensitive to input which uniform grids perform well enough and don't need special design for the input generating. What's more, it can auto-detect the noisy input.
- **For the paper 2**, we learn that deep learning provides a way to pricing and calibration of both vanilla and exotic options under both diffusion and pure jump processes with and without stochastic volatility. For American option pricing, RNNs provide a much faster and similar precise way than FFNNs.
- **It is worth mentioning that** there are some recent papers that provide the hardware and training time they used but not all papers did so. The two papers we used just mention the training time without providing their hardware information. We hope that in the future they will provide more detailed information to make it easier for us to repeat their experiments.

# Reference:
1.[Deep learning for ranking response surfaces with applications to optimal stopping problems](https://www-tandfonline-com.ezproxy.cul.columbia.edu/doi/full/10.1080/14697688.2020.1741669)
2.[Supervised Deep Neural Networks (DNNs) for Pricing/Calibration of Vanilla/Exotic Options Under Various Different Processes](https://arxiv.org/abs/1902.05810)

## Contact Me
If you have any questions on this page, please contact me via rn2498@columbia.edu.

This the blog for midterm seminar. My teammate is Yuting Liu.
