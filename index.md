## COMS 6998 Mid-term Seminar Blog
# Deep Learning in Option Pricing and Optimal Stopping Problem

This blog shows what I've learnt from the two papers about deep learning in option pricing.

# Motivation
High-dimensional optimal stopping problems are notoriously difficult to solve due to the well-known curse of dimensionality. The input data is often high dimension since there might be multiple underlying assets in a hedging portfolio so that almost all of them do not have calculation methods to gain a explicit answer. 
Lots of research try to use different input design and analyse approximation methods to solve it while the deep learning offers not only an approximation of an optimal strategy but also the optimal expected payoff associated to the considered optimal stopping problem. 

# Background
**1. For optimal stopping problem:** 
- The previous work is trying to use delicate input to better calculate the optimal stopping while the deep learning algorithm is insensitive to the input. The uniform sampling is more preferable.
- Avoid the problem of unaffordable computing time for high-dimensional problem
- Detection of wrong inputs with noise

**2. For pricing/calibration of options:**
- Besides geometric Brownian motion(GBM), variance gamma(VG) model is also commonly use in option pricing so the author want to explore the potential of combine those two.
- Other than the simple European options and American options, Barrier options are more complicated with several more condition to execute and the author also consider about pricing those type of option.

# Paper 1: Deep Learning in Optimal Stopping
## 1.1 Introduction
The paper is mainly about using deep learning algorithms for ranking response surfaces with applications to optimal stopping problems. The surface ranking problem consists in assigning the index of the minimal surface to every input x in the entire (usually continuous) space X, i.e. finding the classifer

![image](https://user-images.githubusercontent.com/55861244/100691900-b06c7000-3357-11eb-9f54-e32590400a7a.png)

The function we want to minimize is a priori unknown but can be noisily sampled, i.e. for any x, one can access the function through its stochastic sampler Y:

![image](https://user-images.githubusercontent.com/55861244/100692175-430d0f00-3358-11eb-9cc3-b9a553a48147.png)

where the last part epsilons are independen random variables with mean zero and variance ![image](https://user-images.githubusercontent.com/55861244/100694501-3b9c3480-335d-11eb-82a2-0f27201a3315.png).
Since we want to estimate C(x) using eep learning algorithms we evaluate the differences using the following loss metric:

![image](https://user-images.githubusercontent.com/55861244/100692361-aac35a00-3358-11eb-8376-50d92de2e635.png)

where the lambda(dx) is a probability measure on X specifying the relative importance of ranking different regions. Here when we evaluate the training accuracy and the generalization accuracy, the loss metric is using the uniform measure ![image](https://user-images.githubusercontent.com/55861244/100694560-55d61280-335d-11eb-9496-566bab3ef05e.png).

## 1.2 Neural Networks and Deep Learning Algorithms
### Input and Output
The input for the the neural networks is 
![image](https://user-images.githubusercontent.com/55861244/100694596-671f1f00-335d-11eb-8292-b4ae6391b391.png)
where J corresponds to the number of points and d is the dimensionality of the problem. And the output should take the form:

![image](https://user-images.githubusercontent.com/55861244/100692685-579dd700-3359-11eb-98ad-4cfb4c4c0066.png)

where L is the number of response surfaces, and pjl is the probability that the l th surface is the minimal at the j th point.
### Architecture
The author uses two types of neural networks which is feed-forward NNs and UNet:

![image](https://user-images.githubusercontent.com/55861244/100692967-edd1fd00-3359-11eb-8f5f-c35d5d5a8d05.png)

### Optimizer and Activation Function
For the optimizer, the author uses the commonly used stochastic gradient descent method. For activation function, the author used ReLu, sigmoid and softmax function.

## 1.3 Numerical Experiments
### 1.3.1 One-dimensional example
The main purpose of this example is to show that input data generated by uniform grids UNIF(or UNIF+NL) gain higher generalization accuracy than sequential design SD(or SD+NL).
Author did this example is trying to see the deep learning performance with training data generated on uniform grids or the points produced by sequential design.
The author set L=2, X=\[0,1\] and define the noisy responses Y1(x) and Y2(x) as 

![image](https://user-images.githubusercontent.com/55861244/100693431-e19a6f80-335a-11eb-889b-676c4cfb0f0f.png)

Then the true ranking classifier C(x) is comuted as 

![image](https://user-images.githubusercontent.com/55861244/100693636-4e156e80-335b-11eb-9591-7c4f78dcf4dc.png)

Let M be the size of traning data, they conduct their experiments under M=128,256,512 and number of neurons in each hidden layer is set at M/8. For this binary classification problem, they use signoid activation function in last layer and trained the network for 1500 epochs with updating each gradient using M/2 data. They finally get the below result:

![image](https://user-images.githubusercontent.com/55861244/100693951-f88d9180-335b-11eb-8609-9d352cdfddda.png)

The above results show that:
1. for NNs trained by clean data, the training accuracy is higher than the generalization accuracy while for NNs trained by noisy data, the generalization accuracy is higher. This shows that although the training data with noicy labels do not contain 100% accurate information, networks still try to eliminate their effect and adjust them by not training them correctly.
2. The points generating by sequential design contain more errors since they are more concentrated around the true boundaries and fake boundary. This leads to a large number of mis-labeled points and brings lower signal-to-noise ratio.
3. There exists a threshold on the proportion of error in training data so that they can be auto-detect and will not influence the network's predicting accuracy.
After this example, they believes that it is better to use training data generating by uniform grids and there is no need to do the sequential design for deep learning input.

### 1.3.2 Two-dimensional example
The two-dimensional example is aim to study the sensitivity of deep learning algorithms to noisy labels, sampling locations and budget. So the author train the network for a more complex setting with L=5 surfaces and a 2D input space X=\[-2,2\]^2.
The surface, response and training accuracy is showed below:

![image](https://user-images.githubusercontent.com/55861244/100695164-a732d180-335e-11eb-926b-3b8485b4e2ea.png)

The author believes that considering ranking response surfaces as image segmentation allows one to use a broad class of deep neural networks. Since there are no theoretical results in literature indicating which type of architecture works best for specific applications, they tried feed-forward NNs and UNet. 
In this example, they implement the UNet architecture using M=576 uniform grid points with noisy labels, and gain a generalization accuracy of 96.44%, presenting a better performance than feed-forward NNs (corresponding to the 95.1% in table 3), with comparable training time.


### 1.3.3 Ten-dimensional example
The main purpose of this example is to show the robustness of deep learning algorithms. Non-learning algorithms have difficulty in computational time for high dimension. For those example, when NN is trained by clean data without noise, the train accuracy is better while if it is trained by noisy data, the generalization accuracy is better.
In this example, the author use L=3 surfaces and X=[-1,1]^d with d=10. The surfaces they use include embedded Harmann 6-D function, rescaled  

## 1.4 Bermudan Option Pricing

## 1.5 Observations and Insights
- Deep learning method perform quite well with uniform inputs which seems no need to design a special method to generate input and it can also auto-detect the noisy part although it still perform better for the clean data.
- It also provides a solution for high-dimensional problem within affordable computing time.

# Paper 2: Deep Learning in Pricing/Calibration of Vanilla/Exotic Options
## 2.1 Introduction
The authors apply supervised deep neural networks (DNNs) for pricing and calibration of both vanilla and exotic options under both diffusion and pure jump processes with and without stochastic volatility. 
The author consider four different process:
1. Geometric Brownian motion (GBM) 
2. GBM + stochastic arrival (GBMSA) a.k.a Heston stochastic volatility model
3. Variance gamma (VG) model
4. VG + stochastic arrival ï¼ˆVGSA)


## 2.2 Neural Networks and Deep Learning Algorithms
### 2.2.1 Input and Output
They used sample sizes of 300,000 points for training and 60,000 points for validation.
For the input of the network, they use the parameter matrix

![image](https://user-images.githubusercontent.com/55861244/100696806-66d55280-3362-11eb-8388-0aab654bbb96.png)

where all the parameters has the range according to the reality:
- Product and market parameters:

![image](https://user-images.githubusercontent.com/55861244/100697283-98025280-3363-11eb-9073-57927e66a4b6.png)

- Parameters for different model:

![image](https://user-images.githubusercontent.com/55861244/100696856-85d3e480-3362-11eb-8076-66623bce6c26.png)

- **For European Options**, the output y=EC/K, and EC gained using Black-Merton-Scholes formula for GBM process, while EC is obtained by using the Fast Fourier Transform(FFT) algorithm for corresponding input parameter matrices X for VG, GBMSA, VGSA models.
- **For Barrier Options**, since we have put-call parity and relationships between barrier options, the author only consider the Up-and-Out Put Options (UOP). The output is y=UOP/K. UOP is obtained by using the closed form solution for GBM while it is obtained by using Monte Carlo procedure for their corresponding input parameter matrices X for other models.
- **For American Options**, y=AJZ/K where AJZ is obtained by using the Ju-Zhong Approximation.

### 2.2.2 Architecture
They used feed-forward Neural Networks and Convolutional Neural Networks.
After experiments, they choose 120 neurons per layer and 4 layers, which provides lowest RMSE in their experiments.

### 2.2.3 Optimizer and Activation Function
For the optimizer, they first try SGD but suffered from the slow training time and after trying RMSprop and Adam, they finally choose Adam as the optimizer. In their Neural Networks, they use MSE as loss function. After their numerical experiments, they found that the activation of 4 layers with first 3 leaky and last one elu gained the best performance.

## 2.3 Numerical Experiments
### 2.3.1 Validation
In order to validate the model, they check trained model for three cases:
1. **Interpolation(IS):** In this case, they just validate it for interpolated points on our testing set.
2. **Deep-out-of-the-money(DOM):** In this case, they keep everything else fixed, and test how the trained models behave when S0/K is between 0.6 and 0.8.
3. **Longer maturity(LM):**  In this case, they keep everything else fixed, and test how the trained models behave when the maturity is between 3 years and 5 years.
And obtained the below results:
- **For European Options:**

![image](https://user-images.githubusercontent.com/55861244/100698007-6d18fe00-3365-11eb-80c5-33c71667c5f5.png)

- **For Barrier Options:**

![image](https://user-images.githubusercontent.com/55861244/100698082-a05b8d00-3365-11eb-922a-0c00e9736560.png)

- **For American Options:**

![image](https://user-images.githubusercontent.com/55861244/100698692-144a6500-3367-11eb-8f1e-f8afbd748bd8.png)

They observed that with an increase in depth in layers and neurons per layer the accuracy of the predictions increases, but eventually diminishes. Once the training is done, the model can be used for derivative pricing for market parameters both within and reasonably outside the initial range of the training set with very low error.

### 2.3.2 FFNNs VS RNNs
- Although feedforward neural network(FFNN) perform well but it is quite expensive to generate labels for American options. Knowing American options are strongly path-dependent options, we might benefit from recurrent neural networks(RNN) in pricing them. 
- According to the numerical results, the MSE value of FFNN model is slightly smaller than that of the RNN model. 
- On the other hand, there is a big gap between training times of the two models. In our experiment training FFNNs took 50% more time than RNNs. 

## 2.4 Observations and Insights
- Find out that RNNs perform pretty well and also provide a cheap and faster method than FFNNs to price American option. The model can be used for derivative pricing for market parameters both within and reasonably outside the initial range of the training set with very low error.
- Provide a way to not only price the European options and American options, but also the more complicate Barrier options. The model perform well under in-sample test data as well as longer maturity extrapolation data.


# Conclusion
- **For the paper 1**, we learn that deep learning algorithms can be used for ranking response surfaces problems and finding optimal stopping problems, especially for high-dimensional problems. In this application, deep learning algorithms is insensitive to input which unifrom grids perfroms well enough and doesnâ€™t need special design for the input generating. What's more, it can auto-detect the noisy input.
- **For the paper 2**, we learn that deep learning provides a way to pricing and calibration of both vanilla and exotic options under both diffusion and pure jump processes with and without stochastic volatility. For American option pricing, RNNs provides a much faster and similar precise way than FFNNs.
- **It is worth mentioning that** there are some recent papers provide the hardware and training time they used but not all papers did so. The two paper we used just mention the training time without providing their hardware information. We hope that in the future they would provide more detail information to make it easier for us to repeat their experiments.

# Reference:
1.[Deep learning for ranking response surfaces with applications to optimal stopping problems](https://www-tandfonline-com.ezproxy.cul.columbia.edu/doi/full/10.1080/14697688.2020.1741669)
2.[Supervised Deep Neural Networks (DNNs) for Pricing/Calibration of Vanilla/Exotic Options Under Various Different Processes](https://arxiv.org/abs/1902.05810)

## Contact Me
If you have any questions on this page, please contact me via rn2498@columbia.edu.

This the blog for midterm seminar. My teammate is Yuting Liu.
