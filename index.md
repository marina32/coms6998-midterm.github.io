# COMS 6998 Mid-term Seminar Blog
# Deep Learning in Option Pricing and Optimal Stopping Problem

This blog shows what I've learnt from the two papers about deep learning in option pricing.

# Motivation
Option is an important financial derivatives which

High-dimensional optimal stopping problems are notoriously difficult to solve due to the well-known curse of dimensionality. The input data is often high dimension since there might be multiple underlying assets in a hedging portfolio so that almost all of them do not have calculation methods to gain a explicit answer. 
Lots of research try to use different input design and analyse approximation methods to solve it while the deep learning offers not only an approximation of an optimal strategy but also the optimal expected payoff associated to the considered optimal stopping problem. 

# Background
**1. For optimal stopping problem:** 
- The previous work is trying to use delicate input to better calculate the optimal stopping while the deep learning algorithm is insensitive to the input. The uniform sampling is more preferable.
- Avoid the problem of unaffordable computing time for high-dimensional problem
- Detection of wrong inputs with noise

**2. For pricing/calibration of options:**
- Besides geometric Brownian motion(GBM), variance gamma(VG) model is also commonly use in option pricing so the author want to explore the potential of combine those two.
- Other than the simple European options and American options, Barrier options are more complicated with several more condition to execute and the author also consider about pricing those type of option.

# Paper 1: Deep Learning in Optimal Stopping
## 1.1 Introduction
The paper is mainly about using deep learning algorithms for ranking response surfaces with applications to optimal stopping problems. The surface ranking problem consists in assigning the index of the minimal surface to every input x in the entire (usually continuous) space X, i.e. finding the classifer
![image](https://user-images.githubusercontent.com/55861244/100691900-b06c7000-3357-11eb-9f54-e32590400a7a.png)
The function we want to minimize is a priori unknown but can be noisily sampled, i.e. for any x, one can access the function through its stochastic sampler Y:
![image](https://user-images.githubusercontent.com/55861244/100692175-430d0f00-3358-11eb-9cc3-b9a553a48147.png)
where the last part epsilons are independen random variables with mean zero and variance ![image](https://user-images.githubusercontent.com/55861244/100692242-6b950900-3358-11eb-9d64-a222d8f3fffe.png).
Since we want to estimate C(x) using eep learning algorithms we evaluate the differences using the following loss metric:
![image](https://user-images.githubusercontent.com/55861244/100692361-aac35a00-3358-11eb-8376-50d92de2e635.png)
where the lambda(dx) is a probability measure on X specifying the relative importance of ranking different regions. Here when we evaluate the training accuracy and the generalization accuracy, the loss metric is using the uniform measure
![image](https://user-images.githubusercontent.com/55861244/100692832-aa778e80-3359-11eb-892f-049e32f67cd4.png).


## 1.2 Neural Networks and Deep Learning Algorithms
The input for the the neural networks is 
![image](https://user-images.githubusercontent.com/55861244/100692628-3c32cc00-3359-11eb-95ff-9318521d41bb.png)
where J corresponds to the number of points and d is the dimensionality of the problem. And the output should take the form:
![image](https://user-images.githubusercontent.com/55861244/100692685-579dd700-3359-11eb-98ad-4cfb4c4c0066.png)
where L is the number of response surfaces, and pjl is the probability that the l th surface is the minimal at the j th point.
The author uses two types of neural networks which is feed-forward NNs and UNet:
![image](https://user-images.githubusercontent.com/55861244/100692967-edd1fd00-3359-11eb-8f5f-c35d5d5a8d05.png)
For the optimizer, the author uses the commonly used stochastic gradient descent method. For activation function, the author used ReLu, sigmoid and softmax function.

## 1.3 Numerical Experiments
### 1.3.1 One-dimensional example
The main purpose of this example is to show that input data generated by uniform grids UNIF(or UNIF+NL) gain higher generalization accuracy than sequential design SD(or SD+NL).
Author did this example is trying to see the deep learning performance with training data generated on uniform grids or the points produced by sequential design.
The author set L=2, X=\[0,1\] and define the noisy responses Y1(x) and Y2(x) as 
![image](https://user-images.githubusercontent.com/55861244/100693431-e19a6f80-335a-11eb-889b-676c4cfb0f0f.png)


### 1.3.2 Two-dimensional example


### 1.3.3 Ten-dimensional example
The main purpose of this example is to show the robustness of deep learning algorithms. Non-learning algorithms have difficulty in computational time for high dimension. For those example, when NN is trained by clean data without noise, the train accuracy is better while if it is trained by noisy data, the generalization accuracy is better.
In this example 

## 1.4 Bermudan Option Pricing

## 1.5 Observations and Insights
- Deep learning method perform quite well with uniform inputs which seems no need to design a special method to generate input and it can also auto-detect the noisy part although it still perform better for the clean data.
- Provide a solution for high-dimensional problem within affordable computing time 


# Paper 2: Deep Learning in Pricing/Calibration of Vanilla/Exotic Options
## 2.1 Introduction

## 2.2 Neural Networks and Deep Learning Algorithms

## 2.3 Numerical Experiments

## 2.4 Observations and Insights
- Find out that RNNs perform pretty well and also provide a cheap and faster method than FFNNs to price American option. The model can be used for derivative pricing for market parameters both within and reasonably outside the initial range of the training set with very low error.
- Provide a way to not only price the European options and American options, but also the more complicate Barrier options. The model perform well under in-sample test data as well as longer maturity extrapolation data.


```markdown
Syntax highlighted code block

# Header 1
## Header 2
### Header 3

- Bulleted
- List

1. Numbered
2. List
**Bold** and _Italic_ and `Code` text
```

For more details see [GitHub Flavored Markdown](https://guides.github.com/features/mastering-markdown/).


Your Pages site will use the layout and styles from the Jekyll theme you have selected in your [repository settings](https://github.com/marina32/coms6998-midterm.github.io/settings). The name of this theme is saved in the Jekyll `_config.yml` configuration file.

# Conclusion


# Reference:
1.[Deep learning for ranking response surfaces with applications to optimal stopping problems](https://www-tandfonline-com.ezproxy.cul.columbia.edu/doi/full/10.1080/14697688.2020.1741669)
2.[Supervised Deep Neural Networks (DNNs) for Pricing/Calibration of Vanilla/Exotic Options Under Various Different Processes](https://arxiv.org/abs/1902.05810)

## Contact Me
If you have any questions on this page, please contact me via rn2498@columbia.edu.

This the blog for midterm seminar. My teammate is Yuting Liu.
